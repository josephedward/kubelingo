- id: ckad-sim::q8-rollout-undo
  prompt: A deployment named `api-new-c32` in namespace `neptune` is failing after
    a recent update due to an image with a typo (`ngnix` instead of `nginx`). Check
    the deployment's history and roll back to the previous working revision.
  type: live_k8s_edit
  pre_shell_cmds:
  - kubectl create ns neptune
  - kubectl -n neptune create deploy api-new-c32 --image=nginx:1.16.1
  - kubectl -n neptune wait --for=condition=available deploy/api-new-c32 --timeout=60s
  - kubectl -n neptune set image deploy/api-new-c32 nginx=ngnix:1.16.3
  validation_steps:
  - cmd: kubectl -n neptune wait --for=condition=available deploy/api-new-c32 --timeout=30s
    matcher: {}
  - cmd: kubectl -n neptune get deploy api-new-c32 -o jsonpath='{.spec.template.spec.containers[0].image}'
    matcher: {}
  explanation: Use `kubectl -n neptune rollout history deployment <name>` to see revisions.
    To find the error, `kubectl -n neptune describe pod <pod-name>` on one of the
    failing pods will show an `ImagePullBackOff` error. To fix this, use `kubectl
    -n neptune rollout undo deployment <name>` to revert to the previous, working
    revision.
  metadata:
    id: ckad-sim::q8-rollout-undo
    type: live_k8s_edit
    category: Workload Management
    source: https://killer.sh/ckad
- id: ckad-sim::q9-pod-to-deployment
  prompt: 'A single pod `holy-api` is running in namespace `pluto`. Convert it into
    a high-availability Deployment named `holy-api` with 3 replicas. The original
    pod''s definition is available at `/opt/course/9/holy-api-pod.yaml`. The new Deployment
    must add a `securityContext` to the container, setting `allowPrivilegeEscalation:
    false` and `privileged: false`. Save the final Deployment manifest to `/opt/course/9/holy-api-deployment.yaml`,
    create the Deployment, and delete the original pod.'
  type: live_k8s_edit
  pre_shell_cmds:
  - kubectl create ns pluto
  - mkdir -p /opt/course/9
  - "cat <<'EOF' > /opt/course/9/holy-api-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n\
    \  labels:\n    id: holy-api\n  name: holy-api\nspec:\n  containers:\n  - env:\n\
    \    - name: CACHE_KEY_1\n      value: b&MTCi0=[T66RXm!jO@\n    - name: CACHE_KEY_2\n\
    \      value: PCAILGej5Ld@Q%{Q1=#\n    - name: CACHE_KEY_3\n      value: 2qz-]2OJlWDSTn_;RFQ\n\
    \    image: nginx:1.17.3-alpine\n    name: holy-api-container\n    volumeMounts:\n\
    \    - mountPath: /cache1\n      name: cache-volume1\n    - mountPath: /cache2\n\
    \      name: cache-volume2\n    - mountPath: /cache3\n      name: cache-volume3\n\
    \  volumes:\n  - emptyDir: {}\n    name: cache-volume1\n  - emptyDir: {}\n   \
    \ name: cache-volume2\n  - emptyDir: {}\n    name: cache-volume3\nEOF\n"
  - kubectl -n pluto apply -f /opt/course/9/holy-api-pod.yaml
  validation_steps:
  - cmd: kubectl -n pluto get deploy holy-api -o jsonpath='{.spec.replicas}'
    matcher: {}
  - cmd: kubectl -n pluto wait --for=condition=available deploy/holy-api --timeout=60s
    matcher: {}
  - cmd: kubectl -n pluto get deploy holy-api -o jsonpath='{.spec.template.spec.containers[0].securityContext.allowPrivilegeEscalation}'
    matcher: {}
  - cmd: kubectl -n pluto get deploy holy-api -o jsonpath='{.spec.template.spec.containers[0].securityContext.privileged}'
    matcher: {}
  - cmd: test -f /opt/course/9/holy-api-deployment.yaml
    matcher: {}
  - cmd: '! kubectl -n pluto get pod holy-api'
    matcher: {}
  explanation: 'Start by creating a Deployment manifest based on the pod definition
    in `/opt/course/9/holy-api-pod.yaml`. Change `kind: Pod` to `kind: Deployment`
    and `apiVersion: v1` to `apiVersion: apps/v1`. Wrap the pod''s `spec` and `metadata`
    inside a `template` field. Add `replicas: 3` and a `selector` to the top-level
    `spec`. Add the required `securityContext`. Finally, apply the new manifest and
    delete the old pod.'
  metadata:
    id: ckad-sim::q9-pod-to-deployment
    type: live_k8s_edit
    category: Workload Management
    source: https://killer.sh/ckad
- id: ckad-sim::q3-job
  prompt: 'Create a Job manifest at `/opt/course/3/job.yaml`. The Job, named `neb-new-job`
    in namespace `neptune`, should run the command `sleep 2 && echo done` with image
    `busybox:1.31.0`. It should have a total of 3 completions, with 2 running in parallel.
    Pods created by the Job must have the label `id: awesome-job`, and the container
    should be named `neb-new-job-container`. After creating the manifest, apply it
    to start the Job.'
  type: live_k8s_edit
  pre_shell_cmds:
  - kubectl create namespace neptune
  - mkdir -p /opt/course/3
  validation_steps:
  - cmd: kubectl -n neptune get job neb-new-job -o jsonpath='{.spec.completions}'
    matcher: {}
  - cmd: kubectl -n neptune get job neb-new-job -o jsonpath='{.spec.parallelism}'
    matcher: {}
  - cmd: kubectl -n neptune get job neb-new-job -o jsonpath='{.spec.template.metadata.labels.id}'
    matcher: {}
  - cmd: kubectl -n neptune get job neb-new-job -o jsonpath='{.spec.template.spec.containers[0].name}'
    matcher: {}
  - cmd: test -f /opt/course/3/job.yaml
    matcher: {}
  - cmd: kubectl -n neptune get job neb-new-job
    matcher: {}
  explanation: A Job manifest can be created using `kubectl create job --dry-run=client
    -o yaml`. The YAML should then be edited to set `completions`, `parallelism`,
    container name, and pod labels in the `template` section. The job is then created
    using `kubectl apply -f <file.yaml>`. The job's status can be monitored with `kubectl
    get job` and `kubectl describe job`.
  metadata:
    id: ckad-sim::q3-job
    type: live_k8s_edit
    category: Workload Management
    source: https://killer.sh/ckad
