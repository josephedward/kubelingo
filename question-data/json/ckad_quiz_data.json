[
  {
    "category": "Pod Management",
    "prompts": [
      {
        "prompt": "Create a pod named nginx using the nginx image and expose port 80",
        "response": "kubectl run nginx --image=nginx --port=80"
      },
      {
        "prompt": "Create a pod named busybox that runs the command 'sleep 3600'",
        "response": "kubectl run busybox --image=busybox -- sleep 3600"
      },
      {
        "prompt": "Generate a YAML definition file for a pod named nginx without creating it",
        "response": "kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx.yaml"
      },
      {
        "prompt": "Create a pod that runs the latest nginx image and sets an environment variable DB_URL=postgresql://db",
        "response": "kubectl run nginx --image=nginx --env=\"DB_URL=postgresql://db\""
      },
      {
        "prompt": "Extract the YAML definition of a running pod named 'webapp' in the 'development' namespace",
        "response": "kubectl get pod webapp -n development -o yaml > webapp.yaml"
      },
      {
        "prompt": "Create a pod named nginx using the nginx image and set labels app=web and tier=frontend",
        "response": "kubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\""
      },
      {
        "prompt": "Create an interactive temporary pod with Ubuntu image to troubleshoot cluster issues",
        "response": "kubectl run my-shell --rm -i --tty --image=ubuntu -- bash"
      },
      {
        "prompt": "Create a pod with resource requests of 100m CPU and 256Mi memory",
        "response": "kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml # Then add requests specification",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Create a pod with resource requests of 100m CPU and 256Mi memory"
      },
      {
        "prompt": "Create a pod that runs curl against an internal service at 10.244.0.4",
        "response": "kubectl run curl1 --image=curlimages/curl -i -t --rm --restart=Never -- curl 10.244.0.4"
      },
      {
        "prompt": "Q: Create a Pod named `config-pod` using the `busybox` image that mounts a ConfigMap named `app-config` as a volume at `/etc/config`",
        "response": "kubectl run config-pod --image=busybox --dry-run=client -o yaml > pod.yaml # Then add volume mount configuration",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Create a Pod named `config-pod` using the `busybox` image that mounts a ConfigMap named `app-config` as a volume at `/etc/config`"
      }
    ]
  },
  {
    "category": "Deployment Management",
    "prompts": [
      {
        "prompt": "Create a deployment named webapp with image nginx:1.17 and 3 replicas",
        "response": "kubectl create deployment webapp --image=nginx:1.17 --replicas=3"
      },
      {
        "prompt": "Generate a YAML file for a deployment with nginx image without creating it",
        "response": "kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > deploy.yaml"
      },
      {
        "prompt": "Scale a deployment named 'frontend' to 5 replicas",
        "response": "kubectl scale deployment frontend --replicas=5"
      },
      {
        "prompt": "Update the image of a deployment named 'webapp' to nginx:1.18",
        "response": "kubectl set image deployment/webapp webapp=nginx:1.18"
      },
      {
        "prompt": "Check the rollout status of a deployment named 'frontend'",
        "response": "kubectl rollout status deployment/frontend"
      },
      {
        "prompt": "Roll back a deployment named 'webapp' to its previous version",
        "response": "kubectl rollout undo deployment/webapp"
      },
      {
        "prompt": "Create a deployment with a record of the change-cause for future reference",
        "response": "kubectl create deployment nginx --image=nginx --record"
      },
      {
        "prompt": "View the history of a deployment named 'webapp'",
        "response": "kubectl rollout history deployment/webapp"
      },
      {
        "prompt": "Pause the rollout of a deployment named 'frontend'",
        "response": "kubectl rollout pause deployment/frontend"
      },
      {
        "prompt": "Resume the rollout of a deployment named 'frontend'",
        "response": "kubectl rollout resume deployment/frontend"
      }
    ]
  },
  {
    "category": "Namespace Operations",
    "prompts": [
      {
        "prompt": "Create a new namespace called 'development'",
        "response": "kubectl create namespace development"
      },
      {
        "prompt": "Set the current context to use the 'production' namespace by default",
        "response": "kubectl config set-context --current --namespace=production",
        "review": true
      },
      {
        "prompt": "Verify which namespace is currently being used in your context",
        "response": "kubectl config view --minify | grep namespace:",
        "review": true
      },
      {
        "prompt": "Create a new namespace called 'testing' and output its YAML definition",
        "response": "kubectl create ns testing --dry-run=client -o yaml"
      },
      {
        "prompt": "List all resources across all namespaces",
        "response": "kubectl get all -A",
        "review": true
      },
      {
        "prompt": "List all pods in the 'kube-system' namespace",
        "response": "kubectl get pods -n kube-system"
      },
      {
        "prompt": "Create a pod in the 'development' namespace",
        "response": "kubectl run nginx --image=nginx -n development",
        "review": true
      },
      {
        "prompt": "Delete a namespace called 'testing' and all its resources",
        "response": "kubectl delete namespace testing"
      },
      {
        "prompt": "List all namespaces in the cluster",
        "response": "kubectl get namespaces"
      },
      {
        "prompt": "Create a resource quota for a namespace limiting it to 10 pods",
        "response": "kubectl create quota ns-quota --hard=pods=10 -n development",
        "review": true
      }
    ]
  },
  {
    "category": "ConfigMap Operations",
    "prompts": [
      {
        "prompt": "Create a ConfigMap named 'app-config' with key-value pairs: APP_COLOR=blue and APP_MODE=prod",
        "response": "kubectl create cm app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod"
      },
      {
        "prompt": "Create a ConfigMap named 'game-config' from a configuration file located at '/configs/game.properties'",
        "response": "kubectl create cm game-config --from-file=/configs/game.properties"
      },
      {
        "prompt": "View the contents of a ConfigMap named 'db-config'",
        "response": "kubectl describe cm db-config"
      },
      {
        "prompt": "Create a ConfigMap from all files in the '/configs' directory",
        "response": "kubectl create cm app-settings --from-file=/configs",
        "review": true
      },
      {
        "prompt": "Create a pod that uses a ConfigMap as environment variables",
        "response": "kubectl run app --image=nginx --dry-run=client -o yaml > pod.yaml # Then add envFrom with configMapRef",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Create a pod that uses a ConfigMap as environment variables"
      },
      {
        "prompt": "Extract a ConfigMap to YAML format",
        "response": "kubectl get cm app-config -o yaml > config.yaml"
      },
      {
        "prompt": "Update a value in an existing ConfigMap named 'app-config'",
        "response": "kubectl edit cm app-config"
      },
      {
        "prompt": "Create a ConfigMap named 'multi-config' with key-value pairs DB_URL=mysql://db, API_KEY=123456, and DEBUG=true from the command line",
        "response": "kubectl create cm multi-config --from-literal=DB_URL=mysql://db --from-literal=API_KEY=123456 --from-literal=DEBUG=true"
      },
      {
        "prompt": "Mount a ConfigMap as a volume in a pod at path /etc/config with specific file permissions",
        "response": "kubectl run config-pod --image=nginx --dry-run=client -o yaml > pod.yaml # Then add volume with ConfigMap and defaultMode",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Mount a ConfigMap as a volume in a pod at path /etc/config with specific file permissions"
      },
      {
        "prompt": "Create a ConfigMap from an env file",
        "response": "kubectl create cm env-config --from-env-file=config.env"
      }
    ]
  },
  {
    "category": "Secret Management",
    "prompts": [
      {
        "prompt": "Create a Secret named 'db-creds' with username=admin and password=password123",
        "response": "kubectl create secret generic db-creds --from-literal=username=admin --from-literal=password=password123"
      },
      {
        "prompt": "Create a Secret named 'tls-cert' from a certificate file",
        "response": "kubectl create secret generic tls-cert --from-file=/path/to/cert.pem"
      },
      {
        "prompt": "View the encoded values in a Secret named 'api-secrets'",
        "response": "kubectl get secret api-secrets -o yaml"
      },
      {
        "prompt": "Encode a string for use in a Secret YAML definition",
        "response": "echo -n 'string' | base64"
      },
      {
        "prompt": "Decode a base64 encoded Secret value",
        "response": "echo -n 'encoded string...' | base64 --decode"
      },
      {
        "prompt": "Create a TLS Secret type from certificate and key files",
        "response": "kubectl create secret tls my-tls --cert=path/to/cert.crt --key=path/to/key.key"
      },
      {
        "prompt": "Create a pod that uses a Secret value as an environment variable",
        "response": "kubectl run secure-app --image=nginx --dry-run=client -o yaml > pod.yaml # Then add env with secretKeyRef",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Create a pod that uses a Secret value as an environment variable"
      },
      {
        "prompt": "Create a Secret from a .env file",
        "response": "kubectl create secret generic my-secrets --from-env-file=.env"
      },
      {
        "prompt": "Create a pod that mounts a Secret as a volume using the nginx image",
        "response": "kubectl run secret-pod --image=nginx --dry-run=client -o yaml > pod.yaml # add volume.secret",
        "yaml_exercise": true,
        "exercise_type": "pod",
        "requirements": "Create a pod that mounts a Secret as a volume using the nginx image"
      },
      {
        "prompt": "Create a docker-registry type Secret for private registry authentication",
        "response": "kubectl create secret docker-registry regcred --docker-server=REGISTRY_SERVER --docker-username=USERNAME --docker-password=PASSWORD --docker-email=EMAIL"
      }
    ]
  },
  {
    "category": "Service Account Operations",
    "prompts": [
      {
        "prompt": "Create a Service Account named 'deployment-sa'",
        "response": "kubectl create sa deployment-sa"
      },
      {
        "prompt": "List all Service Accounts in the current namespace",
        "response": "kubectl get sa"
      }
    ]
  },
  {
    "category": "Additional Commands",
    "prompts": [
      {
        "prompt": "Create an alias 'k' for 'kubectl'",
        "response": "alias k=kubectl"
      },
      {
        "prompt": "Enable kubectl autocompletion for bash",
        "response": "source <(kubectl completion bash)"
      },
      {
        "prompt": "Add kubectl autocompletion to ~/.bashrc",
        "response": "echo \"source <(kubectl completion bash)\" >> ~/.bashrc"
      },
      {
        "prompt": "Set the KUBECONFIG environment variable to /home/user/config",
        "response": "export KUBECONFIG=/home/user/config"
      },
      {
        "prompt": "Check the kubectl client and server versions",
        "response": "kubectl version"
      },
      {
        "prompt": "Create a namespace named dev",
        "response": "kubectl create namespace dev"
      },
      {
        "prompt": "List all namespaces",
        "response": "kubectl get namespaces"
      },
      {
        "prompt": "Describe namespace dev",
        "response": "kubectl describe namespace dev"
      },
      {
        "prompt": "Delete namespace dev",
        "response": "kubectl delete namespace dev"
      },
      {
        "prompt": "Create a deployment named frontend using image nginx:1.14",
        "response": "kubectl create deployment frontend --image=nginx:1.14"
      },
      {
        "prompt": "List all deployments",
        "response": "kubectl get deployments"
      },
      {
        "prompt": "Describe deployment frontend",
        "response": "kubectl describe deployment frontend"
      },
      {
        "prompt": "Scale deployment frontend to 3 replicas",
        "response": "kubectl scale deployment frontend --replicas=3"
      },
      {
        "prompt": "Roll back the deployment frontend to the previous version",
        "response": "kubectl rollout undo deployment frontend"
      },
      {
        "prompt": "Restart the deployment frontend",
        "response": "kubectl rollout restart deployment frontend"
      },
      {
        "prompt": "Delete deployment frontend",
        "response": "kubectl delete deployment frontend"
      },
      {
        "prompt": "Run a pod named busybox with image busybox that runs indefinitely",
        "response": "kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c \"sleep 3600\""
      },
      {
        "prompt": "List all pods",
        "response": "kubectl get pods"
      },
      {
        "prompt": "Describe pod busybox",
        "response": "kubectl describe pod busybox"
      },
      {
        "prompt": "Delete pod busybox",
        "response": "kubectl delete pod busybox"
      },
      {
        "prompt": "Expose deployment frontend on port 80 as a ClusterIP service named frontend-svc",
        "response": "kubectl expose deployment frontend --port=80 --type=ClusterIP --name=frontend-svc"
      },
      {
        "prompt": "List all services",
        "response": "kubectl get services"
      },
      {
        "prompt": "Describe service frontend-svc",
        "response": "kubectl describe service frontend-svc"
      },
      {
        "prompt": "Delete service frontend-svc",
        "response": "kubectl delete service frontend-svc"
      },
      {
        "prompt": "Create a ConfigMap named app-config from file config.yaml",
        "response": "kubectl create configmap app-config --from-file=config.yaml"
      },
      {
        "prompt": "List all ConfigMaps",
        "response": "kubectl get configmaps"
      },
      {
        "prompt": "Describe configmap app-config",
        "response": "kubectl describe configmap app-config"
      },
      {
        "prompt": "Delete configmap app-config",
        "response": "kubectl delete configmap app-config"
      },
      {
        "prompt": "Create a generic secret named db-secret with literal username and password",
        "response": "kubectl create secret generic db-secret --from-literal=username=admin --from-literal=password=secret"
      },
      {
        "prompt": "List all secrets",
        "response": "kubectl get secrets"
      },
      {
        "prompt": "Describe secret db-secret",
        "response": "kubectl describe secret db-secret"
      },
      {
        "prompt": "Delete secret db-secret",
        "response": "kubectl delete secret db-secret"
      },
      {
        "prompt": "Create a ServiceAccount named default-sa",
        "response": "kubectl create serviceaccount default-sa"
      },
      {
        "prompt": "List all ServiceAccounts",
        "response": "kubectl get serviceaccounts"
      },
      {
        "prompt": "Describe serviceaccount default-sa",
        "response": "kubectl describe serviceaccount default-sa"
      },
      {
        "prompt": "Delete serviceaccount default-sa",
        "response": "kubectl delete serviceaccount default-sa"
      },
      {
        "prompt": "Label pod busybox with env=prod",
        "response": "kubectl label pod busybox env=prod"
      },
      {
        "prompt": "Remove the label env from pod busybox",
        "response": "kubectl label pod busybox env-"
      },
      {
        "prompt": "Annotate deployment frontend with description=test",
        "response": "kubectl annotate deployment frontend description=test"
      },
      {
        "prompt": "Remove the annotation description from deployment frontend",
        "response": "kubectl annotate deployment frontend description-"
      },
      {
        "prompt": "Apply the configuration in deployment.yaml",
        "response": "kubectl apply -f deployment.yaml"
      },
      {
        "prompt": "Replace configmap using file configmap.yaml",
        "response": "kubectl replace -f configmap.yaml"
      },
      {
        "prompt": "Open the file app.yaml in Vim",
        "response": "vim app.yaml"
      },
      {
        "prompt": "Use kubectl to edit deployment frontend",
        "response": "kubectl edit deployment frontend"
      },
      {
        "prompt": "Open service.yaml in Vim",
        "response": "vim service.yaml"
      },
      {
        "prompt": "View logs of pod busybox",
        "response": "kubectl logs busybox"
      },
      {
        "prompt": "Follow logs of pod busybox",
        "response": "kubectl logs busybox -f"
      },
      {
        "prompt": "View logs of container nginx in pod frontend-pod",
        "response": "kubectl logs frontend-pod -c nginx"
      },
      {
        "prompt": "Execute a shell in pod busybox",
        "response": "kubectl exec -it busybox -- /bin/sh"
      },
      {
        "prompt": "Port-forward local port 8080 to port 80 on service frontend-svc",
        "response": "kubectl port-forward service/frontend-svc 8080:80"
      },
      {
        "prompt": "Describe pod busybox for troubleshooting",
        "response": "kubectl describe pod busybox"
      },
      {
        "prompt": "List all events",
        "response": "kubectl get events"
      },
      {
        "prompt": "Get pod busybox in YAML format",
        "response": "kubectl get pods busybox -o yaml"
      },
      {
        "prompt": "Get pod busybox in JSON format",
        "response": "kubectl get pods busybox -o json"
      },
      {
        "prompt": "Copy file /tmp/data from pod busybox to current directory",
        "response": "kubectl cp busybox:/tmp/data ./"
      },
      {
        "prompt": "Show resource usage of pods",
        "response": "kubectl top pods"
      },
      {
        "prompt": "Show resource usage of nodes",
        "response": "kubectl top nodes"
      },
      {
        "prompt": "Describe node worker-node-1",
        "response": "kubectl describe node worker-node-1"
      },
      {
        "prompt": "View the current kubectl context",
        "response": "kubectl config current-context"
      },
      {
        "prompt": "List all contexts",
        "response": "kubectl config get-contexts"
      },
      {
        "prompt": "Switch kubectl context to minikube",
        "response": "kubectl config use-context minikube"
      },
      {
        "prompt": "Set the namespace to dev in the current context",
        "response": "kubectl config set-context --current --namespace=dev"
      },
      {
        "prompt": "Display cluster info",
        "response": "kubectl cluster-info"
      },
      {
        "prompt": "Show available API resource types",
        "response": "kubectl api-resources"
      },
      {
        "prompt": "Explain the fields of pods",
        "response": "kubectl explain pods"
      },
      {
        "prompt": "Get pods with label app=frontend",
        "response": "kubectl get pods -l app=frontend"
      },
      {
        "prompt": "Get pods with all labels",
        "response": "kubectl get pods --show-labels"
      },
      {
        "prompt": "Apply deployment.yaml without sending to server (dry run)",
        "response": "kubectl apply -f deployment.yaml --dry-run=client"
      },
      {
        "prompt": "Generate YAML for deployment 'test' using the nginx image without creating it",
        "response": "kubectl create deployment test --image=nginx --dry-run=client -o yaml"
      },
      {
        "prompt": "Continuously watch pods",
        "response": "kubectl get pods -w"
      },
      {
        "prompt": "Get all resources in all namespaces",
        "response": "kubectl get all -A"
      },
      {
        "prompt": "List pods with status phase=Running",
        "response": "kubectl get pods --field-selector=status.phase=Running"
      },
      {
        "prompt": "Watch the rollout status of deployment frontend",
        "response": "kubectl rollout status deployment/frontend -w"
      },
      {
        "prompt": "Restart the deployment frontend",
        "response": "kubectl rollout restart deployment/frontend"
      },
      {
        "prompt": "Auto-scale deployment frontend to minimum 2 and maximum 5 pods at 80% CPU",
        "response": "kubectl autoscale deployment frontend --min=2 --max=5 --cpu-percent=80"
      },
      {
        "prompt": "Check if you can create pods",
        "response": "kubectl auth can-i create pods"
      },
      {
        "prompt": "Show only context names",
        "response": "kubectl config get-contexts -o name"
      },
      {
        "prompt": "Open or create the file app.yaml in Vim",
        "response": "vim app.yaml"
      },
      {
        "prompt": "Enter insert mode",
        "response": "i"
      },
      {
        "prompt": "Exit insert mode",
        "response": "<Esc>"
      },
      {
        "prompt": "Save and quit Vim",
        "response": ":wq"
      },
      {
        "prompt": "Quit without saving",
        "response": ":q!"
      },
      {
        "prompt": "Delete the current line",
        "response": "dd"
      },
      {
        "prompt": "Yank (copy) the current line",
        "response": "yy"
      },
      {
        "prompt": "Paste after the cursor",
        "response": "p"
      },
      {
        "prompt": "Search forward for 'pattern'",
        "response": "/pattern"
      },
      {
        "prompt": "Go to line N",
        "response": ":N"
      },
      {
        "prompt": "Undo the last change",
        "response": "u"
      },
      {
        "prompt": "Redo the last undone change",
        "response": "<C-r>"
      },
      {
        "prompt": "Enter visual mode",
        "response": "v"
      },
      {
        "prompt": "Enter visual line mode",
        "response": "V"
      },
      {
        "prompt": "Enter visual block mode",
        "response": "<C-v>"
      },
      {
        "prompt": "Enter replace mode",
        "response": "R"
      },
      {
        "prompt": "Move to the next word",
        "response": "w"
      },
      {
        "prompt": "Move to the previous word",
        "response": "b"
      },
      {
        "prompt": "Move to the end of the word",
        "response": "e"
      },
      {
        "prompt": "Move to the first character of the line",
        "response": "0"
      },
      {
        "prompt": "Move to the end of the line",
        "response": "$"
      },
      {
        "prompt": "Jump to matching bracket",
        "response": "%"
      }
    ]
  },
  {
    "category": "Live Kubernetes Questions",
    "prompts": [
      {
        "type": "live_k8s_edit",
        "prompt": "The provided YAML is for a basic pod. Modify it to create a pod named 'nginx-live' that uses the 'nginx:stable' image.",
        "starting_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: main\n    image: busybox\n    command: [\"sleep\", \"3600\"]",
        "assert_script": "#!/bin/bash\nset -e\n# Check pod name\nPOD_NAME=$(kubectl get pods -o jsonpath='{.items[0].metadata.name}')\nif [ \"$POD_NAME\" != \"nginx-live\" ]; then\n  echo \"Error: Pod name is '$POD_NAME', expected 'nginx-live'\"\n  exit 1\nfi\n# Check image\nIMAGE=$(kubectl get pod nginx-live -o jsonpath='{.spec.containers[0].image}')\nif [ \"$IMAGE\" != \"nginx:stable\" ]; then\n  echo \"Error: Image is '$IMAGE', expected 'nginx:stable'\"\n  exit 1\nfi\necho \"Pod 'nginx-live' with image 'nginx:stable' found.\"",
        "explanation": "To solve this, you need to change the `metadata.name` to 'nginx-live' and the `spec.containers[0].image` to 'nginx:stable'."
      },
      {
        "type": "live_k8s_edit",
        "prompt": "A 'webapp' deployment is defined in the file. Add a new NodePort service named 'webapp-svc' that exposes the deployment on port 80. The service should target port 8080 on the pods.",
        "starting_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  labels:\n    app: webapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: httpd\n        ports:\n        - containerPort: 8080\n---\n# Add your Service definition below",
        "assert_script": "#!/bin/bash\nset -e\nkubectl wait --for=condition=available deployment/webapp --timeout=60s > /dev/null\n# Check service exists and is NodePort\nTYPE=$(kubectl get svc webapp-svc -o jsonpath='{.spec.type}')\nif [ \"$TYPE\" != \"NodePort\" ]; then\n  echo \"Error: Service 'webapp-svc' is type '$TYPE', expected 'NodePort'\"\n  exit 1\nfi\n# Check service port and targetPort\nPORT=$(kubectl get svc webapp-svc -o jsonpath='{.spec.ports[0].port}')\nTARGET_PORT=$(kubectl get svc webapp-svc -o jsonpath='{.spec.ports[0].targetPort}')\nif [ \"$PORT\" != \"80\" ]; then\n  echo \"Error: Service port is '$PORT', expected '80'\"\n  exit 1\nfi\nif [ \"$TARGET_PORT\" != \"8080\" ]; then\n  echo \"Error: Service targetPort is '$TARGET_PORT', expected '8080'\"\n  exit 1\nfi\necho \"Service 'webapp-svc' correctly configured as a NodePort.\"",
        "explanation": "You need to add a second resource definition for the Service, setting `spec.type` to `NodePort`, `spec.ports[0].port` to 80, `spec.ports[0].targetPort` to 8080, and `spec.selector` to match the deployment's labels (`app: webapp`)."
      },
      {
        "type": "live_k8s_edit",
        "prompt": "Create two resources: 1. A ConfigMap named 'app-config' with data 'index.html: <h1>Hello World</h1>'. 2. A Pod named 'web-server' using the 'nginx' image that mounts the ConfigMap as a volume at '/usr/share/nginx/html', replacing the default nginx page.",
        "starting_yaml": "# Add your ConfigMap and Pod definitions here",
        "assert_script": "#!/bin/bash\nset -e\n# Check ConfigMap data\nDATA=$(kubectl get cm app-config -o jsonpath='{.data.index\\.html}')\nif ! echo \"$DATA\" | grep -q 'Hello World'; then\n  echo \"Error: ConfigMap 'app-config' does not have the correct data.\"\n  exit 1\nfi\n# Wait for pod to be ready\nkubectl wait --for=condition=Ready pod/web-server --timeout=120s > /dev/null\n# Check that the mounted file contains the correct content\nCONTENT=$(kubectl exec web-server -- curl -s localhost)\nif ! echo \"$CONTENT\" | grep -q 'Hello World'; then\n  echo \"Error: The web server did not return the expected content from the ConfigMap.\"\n  echo \"Received: $CONTENT\"\n  exit 1\nfi\necho \"Pod is serving content from the mounted ConfigMap correctly.\"",
        "explanation": "You need to define a ConfigMap with the specified data. Then, define a Pod that references this ConfigMap in `spec.volumes` and mounts it into the container at the correct path using `spec.containers[0].volumeMounts`."
      }
    ]
  },
  {
    "category": "Advanced Scenarios",
    "prompts": [
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container.\nYour manager would like to run a command manually on occasion to output the status of that exact Pod. Please write a command that does this into /opt/course/2/pod1-status-command.sh on ckad5601. The command should use kubectl.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune needs a Job template located at /opt/course/3/job.yaml. This Job should run image busybox:1.31.0 and execute sleep 2 && echo done. It should be in namespace neptune, run a total of 3 times and should execute 2 runs in parallel.\nStart the Job and check its history. Each pod created by the Job should have the label id: awesome-job. The job should be named neb-new-job and the container neb-new-job-container.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Mercury asked you to perform some operations using Helm, all in Namespace mercury:\nDelete release internal-issue-report-apiv1\nUpgrade release internal-issue-report-apiv2 to any newer version of chart bitnami/nginx available\nInstall a new release internal-issue-report-apache of chart bitnami/apache. The Deployment should have two replicas, set these via Helm-values during install\nThere seems to be a broken release, stuck in pending-install state. Find it and delete it",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune has its own ServiceAccount named neptune-sa-v2 in Namespace neptune. A coworker needs the token from the Secret that belongs to that ServiceAccount. Write the base64 decoded token to file /opt/course/5/token on ckad7326.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a single Pod named pod6 in Namespace default of image busybox:1.31.0. The Pod should have a readiness-probe executing cat /tmp/ready. It should initially wait 5 and periodically wait 10 seconds. This will set the container ready only if the file /tmp/ready exists.\nThe Pod should run the command touch /tmp/ready && sleep 1d, which will create the necessary file to be ready and then idles. Create the Pod and confirm it starts.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThe board of Team Neptune decided to take over control of one e-commerce webserver from Team Saturn. The administrator who once setup this webserver is not part of the organisation any longer. All information you could get was that the e-commerce system is called my-happy-shop.\nSearch for the correct Pod in Namespace saturn and move it to Namespace neptune. It doesn't matter if you shut it down and spin it up again, it probably hasn't any customers anyways.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThere is an existing Deployment named api-new-c32 in Namespace neptune. A developer did make an update to the Deployment but the updated version never came online. Check the Deployment history and find a revision that works, then rollback to it. Could you tell Team Neptune what the error was so it doesn't happen again?",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nIn Namespace pluto there is single Pod named holy-api. It has been working okay for a while now but Team Pluto needs it to be more reliable.\nConvert the Pod into a Deployment named holy-api with 3 replicas and delete the single Pod once done. The raw Pod template file is available at /opt/course/9/holy-api-pod.yaml.\nIn addition, the new Deployment should set allowPrivilegeEscalation: false and privileged: false for the security context on container level.\nPlease create the Deployment and save its yaml under /opt/course/9/holy-api-deployment.yaml on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Pluto needs a new cluster internal Service. Create a ClusterIP Service named project-plt-6cc-svc in Namespace pluto. This Service should expose a single Pod named project-plt-6cc-api of image nginx:1.17.3-alpine, create that Pod as well. The Pod should be identified by label project: plt-6cc-api. The Service should use tcp port redirection of 3333:80.\nFinally use for example curl from a temporary nginx:alpine Pod to get the response from the Service. Write the response into /opt/course/10/service_test.html on ckad9043. Also check if the logs of Pod project-plt-6cc-api show the request and write those into /opt/course/10/service_test.log on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nThere are files to build a container image located at /opt/course/11/image on ckad9043. The container will run a Golang application which outputs information to stdout. You're asked to perform the following tasks:\n\u2139\ufe0f Run all Docker and Podman commands as user root. Use sudo docker and sudo podman or become root with sudo -i\nChange the Dockerfile: set ENV variable SUN_CIPHER_ID to hardcoded value 5b9c1065-e39d-4a43-a04a-e59bcea3e03f\nBuild the image using sudo docker, tag it registry.killer.sh:5000/sun-cipher:v1-docker and push it to the registry\nBuild the image using sudo podman, tag it registry.killer.sh:5000/sun-cipher:v1-podman and push it to the registry\nRun a container using sudo podman, which keeps running detached in the background, named sun-cipher using image registry.killer.sh:5000/sun-cipher:v1-podman\nWrite the logs your container sun-cipher produces into /opt/course/11/logs on ckad9043",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a new PersistentVolume named earth-project-earthflower-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined.\nNext create a new PersistentVolumeClaim in Namespace earth named earth-project-earthflower-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly.\nFinally create a new Deployment project-earthflower in Namespace earth which mounts that volume at /tmp/project-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Moonpie, which has the Namespace moon, needs more storage. Create a new PersistentVolumeClaim named moon-pvc-126 in that namespace. This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain. The claim should request storage of 3Gi, an accessMode of ReadWriteOnce and should use the new StorageClass.\nThe provisioner moon-retainer will be created by another team, so it's expected that the PVC will not boot yet. Confirm this by writing the event message from the PVC into file /opt/course/13/pvc-126-reason on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nYou need to make changes on an existing Pod in Namespace moon called secret-handler. Create a new Secret secret1 which contains user=test and pass=pwd. The Secret's content should be available in Pod secret-handler as environment variables SECRET1_USER and SECRET1_PASS. The yaml for Pod secret-handler is available at /opt/course/14/secret-handler.yaml.\nThere is existing yaml for another Secret at /opt/course/14/secret2.yaml, create this Secret and mount it inside the same Pod at /tmp/secret2. Your changes should be saved under /opt/course/14/secret-handler-new.yaml on ckad9043. Both Secrets should only be available in Namespace moon.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Moonpie has a nginx server Deployment called web-moon in Namespace moon. Someone started configuring it but it was never completed. To complete please create a ConfigMap called configmap-web-moon-html containing the content of file /opt/course/15/web-moon.html under the data key-name index.html.\nThe Deployment web-moon is already configured to work with this ConfigMap and serve its content. Test the nginx configuration for example using curl from a temporary nginx:alpine Pod.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThe Tech Lead of Mercury2D decided it's time for more logging, to finally fight all these missing data incidents. There is an existing container named cleaner-con in Deployment cleaner in Namespace mercury. This container mounts a volume and writes logs into a file called cleaner.log.\nThe yaml for the existing Deployment is available at /opt/course/16/cleaner.yaml. Persist your changes at /opt/course/16/cleaner-new.yaml on ckad7326 but also make sure the Deployment is running.\nCreate a sidecar container named logger-con, image busybox:1.31.0 , which mounts the same volume and writes the content of cleaner.log to stdout, you can use the tail -f command for this. This way it can be picked up by kubectl logs.\nCheck if the logs of the new container reveal something about the missing data incidents.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nLast lunch you told your coworker from department Mars Inc how amazing InitContainers are. Now he would like to see one in action. There is a Deployment yaml at /opt/course/17/test-init-container.yaml. This Deployment spins up a single Pod of image nginx:1.17.3-alpine and serves files from a mounted volume, which is empty right now.\nCreate an InitContainer named init-con which also mounts that volume and creates a file index.html with content check this out! in the root of the mounted volume. For this test we ignore that it doesn't contain valid html.\nThe InitContainer should be using image busybox:1.31.0. Test your implementation for example using curl from a temporary nginx:alpine Pod.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nThere seems to be an issue in Namespace mars where the ClusterIP service manager-api-svc should make the Pods of Deployment manager-api-deployment available inside the cluster.\nYou can test this with curl manager-api-svc.mars:4444 from a temporary nginx:alpine Pod. Check for the misconfiguration and apply a fix.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nIn Namespace jupiter you'll find an apache Deployment (with one replica) named jupiter-crew-deploy and a ClusterIP Service called jupiter-crew-svc which exposes it. Change this service to a NodePort one to make it available on all nodes on port 30100.\nTest the NodePort Service using the internal IP of all available nodes and the port 30100 using curl, you can reach the internal node IPs directly from your main terminal. On which nodes is the Service reachable? On which node is the Pod running?",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nIn Namespace venus you'll find two Deployments named api and frontend. Both Deployments are exposed inside the cluster using Services. Create a NetworkPolicy named np1 which restricts outgoing tcp connections from Deployment frontend and only allows those going to Deployment api. Make sure the NetworkPolicy still allows outgoing traffic on UDP/TCP ports 53 for DNS resolution.\nTest using: wget www.google.com and wget api:2222 from a Pod of Deployment frontend.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune needs 3 Pods of image httpd:2.4-alpine, create a Deployment named neptune-10ab for this. The containers should be named neptune-pod-10ab. Each container should have a memory request of 20Mi and a memory limit of 50Mi.\nTeam Neptune has its own ServiceAccount neptune-sa-v2 under which the Pods should run. The Deployment should be in Namespace neptune.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Sunny needs to identify some of their Pods in namespace sun. They ask you to add a new label protected: true to all Pods with an existing label type: worker or type: runner. Also add an annotation protected: do not delete this pod to all Pods having the new label protected: true.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      }
    ]
  }
]