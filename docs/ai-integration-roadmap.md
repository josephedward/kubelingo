Here’s a multi-phase roadmap for folding the AI question/manifest generator on top of your existing static engine, while preserving 100% offline/static capability and incrementally adding AI “power‐ups.”  At each phase you’ll get:

  • Objectives
  • Manual steps & example commands
  • What you can do / cannot do yet
  • Skeleton tests you’ll write to lock in correctness

—

## Phase 0: Define & Lock Down Your Canonical Question Schema

Before you touch code, make sure you have a single “question” schema (fields, names, types) that both static YAML and AI JSON will map to.  E.g.:

  • id
  • topic
  • difficulty
  • question
  • expected_resources
  • success_criteria
  • hints
  • scenario_context

Manual

    1. Inspect a few of your existing `/kubelingo/questions/*.yaml` files.  Pick a representative subset.
    2. Document their fields in `docs/question-schema.md`.
    3. Take one of the AI outputs you’ve already generated, dump it to JSON, compare.

Can / Cannot
  • CAN: Agree on canonical field names and types.
  • CANNOT: Import yet—this is purely a design doc exercise.

Tests
  Create tests/phase0/test_schema_consistency.py that

    import yaml, json, pytest

    def test_static_question_has_required_fields():
        q = yaml.safe_load(open("kubelingo/questions/example.yaml"))
        for fld in ("id","topic","difficulty","question","expected_resources","success_criteria"):
            assert fld in q

    def test_ai_question_has_same_fields():
        aiq = json.load(open("scripts/.../sample_questions.json"))[0]
        for fld in ("id","topic","difficulty","question","expected_resources","success_criteria"):
            assert fld in aiq

—

## Phase 1: Normalize All Static Questions to Canonical Schema

Objectives
  • Collapse every static question file into the same YAML structure.
  • Make sure loading/parsing in your static generator still works.

Manual

    1. Write a small script `scripts/normalize_static_questions.py` that reads each YAML in `kubelingo/questions/`, rewrites it with exactly the canonical fields (mapping legacy names if needed).
    2. Run it and commit the normalized output.

Can / Cannot
  • CAN: Continue to generate static quizzes with kubelingo question-manager.
  • CANNOT: Yet use AI to augment or produce questions.

Tests
  Add tests/phase1/test_normalize_static.py:

    from scripts.normalize_static_questions import normalize_all
    import os
    import yaml

    def test_normalize_creates_no_missing_fields(tmp_path):
        normalize_all(src="kubelingo/questions", dst=str(tmp_path))
        for f in os.listdir(tmp_path):
            q = yaml.safe_load(open(os.path.join(tmp_path, f)))
            for fld in ("id","topic","difficulty","question","expected_resources","success_criteria"):
                assert fld in q

—

## Phase 2: Build a “Static × AI” Import Bridge

Objectives
  • Have an independent converter that takes the AI JSON (question_generator.py output) and spits out canonical YAML files in kubelingo/questions/auto/.
  • Do not touch your static code.

Manual

    1. Create `scripts/ai2static.py`:


        * Reads `sample_questions.json`

        * Maps each entry to canonical YAML (you already defined schema)

        * Writes to `kubelingo/questions/auto/<id>.yaml`
    2. Run it on one JSON export:

         python scripts/ai2static.py \
       --input scripts/.../questions.json \
       --dest kubelingo/questions/auto
    3. Inspect a few of the autogenerated YAMLs.

Can / Cannot
  • CAN: Mix static+AI questions in your question‐bank directory.
  • CANNOT: Yet generate on the fly—only via batch conversion.

Tests
  Add tests/phase2/test_ai2static.py:

    import tempfile, os, yaml
    from scripts.ai2static import ai2static

    def test_ai2static_roundtrip(tmp_path):
        # prepare a small JSON with one question
        j = [{
          "id": "abcd1234",
          "topic": "pods",
          "difficulty": "beginner",
          "question": "Test?",
          "expected_resources": ["Pod"],
          "success_criteria": ["YAML syntax is valid"]
        }]
        f = tmp_path/"in.json"
        f.write_text(json.dumps(j))
        outdir = tmp_path/"out"
        ai2static(str(f), str(outdir))
        generated = yaml.safe_load(open(outdir/"abcd1234.yaml"))
        assert generated["id"] == "abcd1234"
        assert "question" in generated

—

## Phase 3: Wire the AI CLI In as an Optional “Question Source”

Objectives
  • Extend your Kubelingo CLI or question‐manager so that you can do kubelingo generate --source=ai --count=5 (in addition to --source=static).
  • Under the hood it shells out to scripts/k8s_manifest_generator.py --mode question ... --output-file tmp.json, then converts to YAML with ai2static.

Manual

    1. In `question_manager.py`, add a new option `--source ai`.
    2. If source==ai:      subprocess.run([
              "python", "../scripts/k8s_manifest_generator.py",
              "--mode", "question",
              "--question-count", str(count),
              "--output-file", tmp_json
           ])
           from scripts.ai2static import ai2static
           ai2static(tmp_json, question_bank_dir)
    3. Leave `--source static` untouched.

Can / Cannot
  • CAN: Run your normal flows offline (static only).
  • CAN: Also run source=ai to bulk-import AI questions.
  • CANNOT: Yet generate on-the-fly per‐session question (that’s Phase 4).

Tests
  Add tests/phase3/test_qmgr_ai_source.py:

    import subprocess, tempfile, os
    def test_qmgr_ai_import(tmp_path, monkeypatch):
        # stub the AI script to output deterministic JSON
        monkeypatch.setenv("K8S_QG_SCRIPT", "tests/fixtures/fake_qg.py")
        out = subprocess.check_output([
          "kubelingo", "generate",
          "--source", "ai",
          "--count", "2",
          "--output-dir", str(tmp_path)
        ]).decode()
        # should have written two YAML files
        assert len(list(tmp_path.glob("*.yaml"))) == 2

—

## Phase 4: On-Demand AI Question Generation (Interactive)

Objectives
  • Allow your CLI to ask the AI for single new questions on the fly:

      kubelingo ask-ai --topic services --difficulty advanced

  • Immediately present the question in your REPL or CLI.

Manual

    1. Add a new command/sub‐command `ask-ai` in `kubelingo.py`.
    2. Shell out similarly to Phase 3 but with `--output-file -` (stdout JSON).
    3. Parse JSON client‐side, pretty‐print in your CLI.

Can / Cannot
  • CAN: Try AI questions live in your CLI.
  • CANNOT: Yet seamlessly drop them into paper quizzes or DB without conversion.

Tests
  Write tests/phase4/test_ask_ai_cli.py that runs your CLI with a fake AI shim returning a known JSON and asserts that the output matches.

—

## Phase 5: AI-powered Manifest Generation & Grading Plugin

Objectives
  • In your “solve” or “verify” steps, allow an AI path:

      kubelingo solve --method ai --question-id Q123  

  • That shells out to k8s_manifest_generator.py --mode generate --prompt "<question>"
  • Collects the YAML, runs your existing validation, plus AI grading under the hood.
  • Falls back to purely static linters if no API key.

Manual

    1. In `solve` command, add `--method ai`.
    2. If method==ai, run the AI generator, capture its YAML.
    3. Pipe that YAML into your existing static validators (`validation.py`), then optionally into AI grader for feedback.

Can / Cannot
  • CAN: Provide students AI-draft solutions & grades.
  • CAN: Still allow --method static to run your current generator + validator.
  • CANNOT: Depend entirely on AI; static always remains available.

Tests

    * `tests/phase5/test_solve_ai_solution.py`
      • monkeypatch the AI script to emit a known pod YAML
      • run `kubelingo solve --method ai --question-id Q123`
      • assert your existing validator returns PASS, and AI grade is attached.

—

## Phase 6: End-to-End Verification & Performance Measurement

Objectives
  • Write E2E smoke tests that combine static and AI flows:
    1. Import 5 static questions + 5 AI questions
    2. Solve each with both static and AI methods
    3. Collect grades & timings
    4. Produce a small report CSV/JSON

Manual

    1. Create `scripts/e2e_report.sh` that orchestrates generating, solving, grading.
    2. Commit it as a demo in `misc/`.

Can / Cannot
  • CAN: Measure AI vs static throughput, average scores.
  • CANNOT: Yet guarantee AI never hallucinate beyond core CKAD scope—keep it “coached” with strong prompts.

Tests

    * `tests/phase6/test_e2e_report.py`: run the script, assert the output JSON has 10 entries with the right fields.

—

### How to Roll This Out

    1. **Sandbox & branch**
       – Create a feature branch `ai-integration`
    2. **Phase 0 → Phase 1**
       – Normalize static questions, write tests, merge once green.
    3. **Phase 2**
       – Build the converter, test it, merge behind a feature flag (`--enable-ai-bridge`).
    4. **Phase 3 → Phase 4**
       – Gradually add CLI flags `--source ai`, `ask-ai` commands, behind flags.
    5. **Phase 5**
       – Offer `solve --method ai` as experimental.
    6. **Phase 6**
       – Turn on E2E dashboards for instructors.

At each merge, your full test suite (static + AI mock tests) must pass.  AI calls are always shimmable via fixtures or fake scripts, so CI never depends on actual OpenAI—only your smoke tests will.

—
That roadmap will let you keep your static backbone intact, iteratively layer AI on top, allow full offline operation, and unlock the rich generative, grading & analytics capabilities of the new system.