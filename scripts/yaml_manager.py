#!/usr/bin/env python3
"""
Single CLI to manage YAML files for Kubelingo, including backups, DB operations, and statistics.
"""
import argparse
import os
import sys
import sqlite3
import datetime
import shutil
import logging
import uuid
import subprocess
import json
import re
import time
from collections import Counter
from dataclasses import asdict
from pathlib import Path
from typing import List, Optional, Dict, Any, Set, Tuple

# Add project root to path to allow imports from kubelingo
try:
    project_root = Path(__file__).resolve().parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    import yaml
    from tqdm import tqdm
    from rich.console import Console
    from rich.progress import track
    import llm

    from kubelingo.database import (
        get_db_connection, add_question, init_db, _row_to_question_dict, get_all_questions
    )
    from kubelingo.question import Question
    from kubelingo.modules.yaml_loader import YAMLLoader
    from kubelingo.modules.ai_categorizer import AICategorizer
    from kubelingo.utils import path_utils
    from kubelingo.utils.path_utils import (
        get_project_root, get_live_db_path, find_yaml_files_from_paths,
        get_all_question_dirs, get_all_yaml_files_in_repo, find_and_sort_files_by_mtime,
        find_yaml_files
    )
    from kubelingo.utils.config import (
        YAML_BACKUP_DIRS, DATABASE_FILE, ENABLED_QUIZZES, QUESTION_DIRS
    )
    from kubelingo.utils.ui import Fore, Style
except ImportError as e:
    print(f"Error: A required module is not available: {e}. "
          "Please ensure all dependencies are installed and you run this from the project root.", file=sys.stderr)
    sys.exit(1)

# --- consolidate_unique_yaml_questions ---
def do_consolidate(args):
    """
    Finds all YAML quiz files, extracts unique questions based on their 'prompt',
    and consolidates them into a single YAML file.
    """
    output_file = Path(args.output) if args.output else \
        Path(project_root) / 'backups' / 'yaml' / f'consolidated_unique_questions_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.yaml'

    print(f"{Fore.CYAN}--- Consolidating unique YAML questions ---{Style.RESET_ALL}")

    try:
        all_yaml_files = get_all_yaml_files_in_repo()
        print(f"Found {len(all_yaml_files)} YAML files to scan in the repository.")
    except Exception as e:
        print(f"{Fore.RED}Error finding YAML files: {e}{Style.RESET_ALL}")
        return

    unique_questions: List[Dict[str, Any]] = []
    seen_prompts: Set[str] = set()
    total_questions_count = 0
    files_with_questions_count = 0

    for file_path in all_yaml_files:
        questions_in_file = []
        try:
            with file_path.open('r', encoding='utf-8') as f:
                documents = yaml.safe_load_all(f)
                for data in documents:
                    if not data:
                        continue
                    if isinstance(data, dict) and 'questions' in data and isinstance(data.get('questions'), list):
                        questions_in_file.extend(data['questions'])
                    elif isinstance(data, list):
                        questions_in_file.extend(data)

        except (yaml.YAMLError, IOError):
            continue

        if questions_in_file:
            files_with_questions_count += 1
            for question in questions_in_file:
                if isinstance(question, dict) and 'prompt' in question:
                    total_questions_count += 1
                    prompt = question.get('prompt')
                    if prompt and prompt not in seen_prompts:
                        seen_prompts.add(prompt)
                        unique_questions.append(question)

    print(f"Scanned {len(all_yaml_files)} YAML files.")
    print(f"Processed {files_with_questions_count} files containing questions.")
    print(f"Found {total_questions_count} questions in total, with {len(unique_questions)} being unique.")

    if not unique_questions:
        print(f"{Fore.YELLOW}No unique questions found to consolidate.{Style.RESET_ALL}")
        return

    output_file.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump({'questions': unique_questions}, f, sort_keys=False, indent=2)
        print(f"\n{Fore.GREEN}Successfully consolidated {len(unique_questions)} unique questions to:{Style.RESET_ALL}")
        print(str(output_file))
    except IOError as e:
        print(f"{Fore.RED}Error writing to output file {output_file}: {e}{Style.RESET_ALL}")


# --- create_quizzes_from_yaml ---
def _process_with_gemini(prompt, model="gemini-2.0-flash"):
    """Uses the llm-gemini plugin to process a prompt with the specified model."""
    try:
        model_instance = llm.get_model(model)
        response = model_instance.prompt(prompt)
        return response.text().strip()
    except Exception as e:
        logging.error(f"Error processing with Gemini: {e}")
        return None

def _add_question_for_create_quizzes(conn, id, prompt, source_file, response, category, source, validation_steps, validator, review):
    """Adds a question to the database, handling JSON serialization for complex fields."""
    try:
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO questions (
                id, prompt, source_file, response, category, source,
                validation_steps, validator, review
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(id) DO UPDATE SET
                prompt=excluded.prompt,
                source_file=excluded.source_file,
                response=excluded.response,
                category=excluded.category,
                source=excluded.source,
                validation_steps=excluded.validation_steps,
                validator=excluded.validator,
                review=excluded.review;
        """, (
            id, prompt, str(source_file), response, category, source,
            json.dumps(validation_steps) if validation_steps is not None else None,
            json.dumps(validator) if validator is not None else None,
            review
        ))
    except sqlite3.Error as e:
        logging.error(f"Failed to add question {id}: {e}")

def do_create_quizzes(args):
    """Indexes YAML files from a consolidated backup and populates the database."""
    logging.info("Starting to create quizzes from consolidated YAML backup.")
    
    proj_root = get_project_root()
    yaml_dir = proj_root / 'yaml'

    logging.info(f"Looking for consolidated question files in: {yaml_dir}")

    if not yaml_dir.is_dir():
        logging.error(f"YAML directory not found at: {yaml_dir}")
        return

    logging.info("Scanning for latest consolidated question file...")
    consolidated_files = sorted(yaml_dir.glob('consolidated_unique_questions_*.yaml'), reverse=True)

    if not consolidated_files:
        logging.warning(f"No 'consolidated_unique_questions_*.yaml' files found in '{yaml_dir}'.")
        return
    
    latest_file = consolidated_files[0]
    yaml_files = [latest_file]

    logging.info(f"Found latest consolidated file. Processing: {latest_file}")
    
    db_path = ":memory:"
    conn = get_db_connection(db_path)
    init_db(db_path=db_path, clear=True)
    logging.info("In-memory database initialized and schema created.")

    question_count = 0
    
    for yaml_file in yaml_files:
        logging.info(f"Processing file: {yaml_file}")
        try:
            with open(yaml_file, 'r') as f:
                data = yaml.safe_load(f)

            if isinstance(data, dict):
                questions_data = data.get('questions')
            else:
                questions_data = data

            if not isinstance(questions_data, list):
                logging.error(f"Skipping file {yaml_file}: Expected a list of questions, but got {type(questions_data)}.")
                continue

            for q_data in questions_data:
                q_id = q_data.get('id')
                q_type = q_data.get('type')
                
                exercise_category = q_type
                if not exercise_category:
                    logging.warning(f"Skipping question {q_id} in {yaml_file}: missing type.")
                    continue
                
                prompt = q_data.get('prompt')
                if not prompt:
                    logging.warning(f"Skipping question {q_id}: Missing 'prompt'.")
                    continue

                if q_type == 'manifest':
                    if 'vim' not in q_data.get('tools', []):
                        logging.warning(f"Skipping manifest question {q_id}: 'vim' tool is required.")
                        continue
                    if 'kubectl apply' not in q_data.get('validation', []):
                        logging.warning(f"Skipping manifest question {q_id}: 'kubectl apply' validation is required.")
                        continue

                _add_question_for_create_quizzes(
                    conn=conn, id=q_id, prompt=prompt, source_file=str(yaml_file),
                    response=q_data.get('response'), category=exercise_category,
                    source=q_data.get('source'), validation_steps=q_data.get('validation'),
                    validator=q_data.get('validator'), review=q_data.get('review', False)
                )
                question_count += 1
                logging.info(f"Added question ID: {q_id} with category '{exercise_category}'.")

        except yaml.YAMLError as e:
            logging.error(f"Error parsing YAML file {yaml_file}: {e}")
        except Exception as e:
            logging.error(f"An unexpected error occurred while processing {yaml_file}: {e}")
    
    if question_count > 0:
        conn.commit()
        logging.info(f"Successfully added {question_count} questions to the in-memory database.")

        live_db_path = Path(get_live_db_path())
        dump_filename = f"quiz_dump_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
        dump_path = live_db_path.parent / dump_filename
        
        with open(dump_path, 'w') as f:
            for line in conn.iterdump():
                f.write(f'{line}\n')
        
        logging.info(f"Database dump created at: {dump_path}")
        logging.info(f"To load this dump into your main database, run:")
        logging.info(f"sqlite3 '{live_db_path}' < '{dump_path}'")
    else:
        logging.info("No new questions were added to the database.")
        
    conn.close()
    logging.info("Quiz creation process finished.")


# --- create_sqlite_db_from_yaml ---
class QuestionSkipped(Exception):
    def __init__(self, message: str, category: Optional[str] = None):
        self.message = message
        self.category = category
        super().__init__(self.message)

def _normalize_and_prepare_question_for_db(q_data: dict, category_to_source_file: dict, allowed_args: set) -> dict:
    q_dict = q_data.copy()
    if "metadata" in q_dict and isinstance(q_dict["metadata"], dict):
        metadata = q_dict.pop("metadata")
        for k, v in metadata.items():
            if k not in q_dict:
                q_dict[k] = v
    if "answer" in q_dict:
        q_dict["correct_yaml"] = q_dict.pop("answer")
    if "starting_yaml" in q_dict:
        q_dict["initial_files"] = {"manifest.yaml": q_dict.pop("starting_yaml")}
    if "question" in q_dict:
        q_dict["prompt"] = q_dict.pop("question")
    q_type = q_dict.get("type")
    if q_type in ("yaml_edit", "yaml_author"):
        if "answer" in q_dict and "correct_yaml" not in q_dict:
            q_dict["correct_yaml"] = q_dict.pop("answer")
        if "starting_yaml" in q_dict and "initial_files" not in q_dict:
            q_dict["initial_files"] = {"f.yaml": q_dict.pop("starting_yaml")}
    if "type" in q_dict:
        q_dict["question_type"] = q_dict.pop("type")
    if "subject" in q_dict:
        q_dict["subject_matter"] = q_dict.pop("subject")
    q_type = q_dict.get("question_type", "command")
    if q_type in ("yaml_edit", "yaml_author", "live_k8s_edit", "manifest"):
        q_dict["schema_category"] = "manifest"
    elif q_type in ("command", "kubectl"):
        q_dict["schema_category"] = "command"
    else:
        q_dict["schema_category"] = "basic"
    if not q_dict.get("category"):
        q_type = q_dict.get("question_type")
        if q_type in ("yaml_edit", "yaml_author"):
            q_dict["category"] = "YAML Authoring"
        elif q_dict.get("subject_matter"):
            q_dict["category"] = q_dict["subject_matter"]
        elif q_dict.get("source") == "AI" and q_dict.get("subject_matter"):
            subject = q_dict.get("subject_matter")
            q_dict["category"] = subject.capitalize()
    category = q_dict.get("category")
    source_file_from_category = category_to_source_file.get(category)
    if source_file_from_category:
        q_dict["source_file"] = source_file_from_category
    elif not q_dict.get("source_file"):
        if category:
            raise QuestionSkipped(f"Unmatched category: {category}", category=category)
        else:
            raise QuestionSkipped("Missing category and could not infer one.")
    q_dict.pop("solution_file", None)
    q_dict.pop("subject", None)
    q_dict.pop("type", None)
    return {k: v for k, v in q_dict.items() if k in allowed_args}

def _populate_db_from_yaml(yaml_files: list[Path], db_path: Optional[str] = None):
    if not yaml_files:
        print("No YAML files found to process.")
        return
    conn = get_db_connection(db_path=db_path)
    allowed_args = {
        "id", "prompt", "source_file", "response", "category", "source",
        "validation_steps", "validator", "review", "question_type",
        "schema_category", "answers", "correct_yaml", "difficulty",
        "explanation", "initial_files", "pre_shell_cmds", "subject_matter", "metadata",
    }
    category_to_source_file = ENABLED_QUIZZES
    unmatched_categories = set()
    skipped_no_category = 0
    question_count = 0
    try:
        for file_path in yaml_files:
            print(f"  - Processing '{file_path.name}'...")
            with open(file_path, "r", encoding="utf-8") as f:
                try:
                    questions_data = yaml.safe_load(f)
                except yaml.YAMLError as e:
                    print(f"Error parsing YAML file {file_path}: {e}", file=sys.stderr)
                    continue
                if not questions_data: continue
                questions_list = questions_data
                if isinstance(questions_data, dict):
                    questions_list = questions_data.get("questions") or questions_data.get("entries")
                if not isinstance(questions_list, list): continue
                for q_data in questions_list:
                    try:
                        q_dict_for_db = _normalize_and_prepare_question_for_db(
                            q_data, category_to_source_file, allowed_args
                        )
                        add_question(conn=conn, **q_dict_for_db)
                        question_count += 1
                    except QuestionSkipped as e:
                        if e.category:
                            unmatched_categories.add(e.category)
                        else:
                            skipped_no_category += 1
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"Error adding questions to database: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        conn.close()
    if unmatched_categories:
        print("\nWarning: The following categories from the YAML file did not match any quiz. Questions in these categories were skipped:")
        for cat in sorted(list(unmatched_categories)):
            print(f"  - {cat}")
    if skipped_no_category > 0:
        print(f"\nWarning: Skipped {skipped_no_category} questions because they were missing a 'category' field.")
    print(f"\nSuccessfully populated database with {question_count} questions.")

def do_create_db(args):
    """Populate the SQLite database from YAML backup files."""
    if args.yaml_files:
        yaml_files = path_utils.find_yaml_files_from_paths(args.yaml_files)
    else:
        print("No input paths provided. Locating most recent YAML backup...")
        all_backups = path_utils.find_and_sort_files_by_mtime(
            YAML_BACKUP_DIRS, extensions=[".yaml", ".yml"]
        )
        if not all_backups:
            print(f"{Fore.RED}Error: No YAML backup files found in configured backup directories.{Style.RESET_ALL}")
            print(f"Searched in: {YAML_BACKUP_DIRS}")
            sys.exit(1)
        latest_backup = all_backups[0]
        print(f"Using most recent backup: {Fore.GREEN}{latest_backup}{Style.RESET_ALL}")
        yaml_files = [latest_backup]
    if not yaml_files:
        print("No YAML files found.")
        sys.exit(0)
    unique_files = sorted(list(set(yaml_files)))
    print(f"Found {len(unique_files)} YAML file(s) to process:")
    if len(unique_files) > 20:
        print("Showing first 10 files:")
        for f in unique_files[:10]: print(f"  - {f.name}")
        print(f"  ...and {len(unique_files) - 10} more.")
    else:
        for f in unique_files: print(f"  - {f.name}")
    db_path = args.db_path or get_live_db_path()
    init_db(clear=args.clear, db_path=db_path)
    print(f"\nPopulating database at: {db_path}")
    _populate_db_from_yaml(unique_files, db_path=db_path)


# --- deduplicate_yaml ---
def _question_to_key(q: Question) -> str:
    """Creates a canonical, hashable key from a Question object for deduplication."""
    d = asdict(q)
    d.pop('id', None)
    d.pop('source_file', None)
    cleaned_dict = {k: v for k, v in d.items() if v is not None}
    return json.dumps(cleaned_dict, sort_keys=True, default=str)

def do_deduplicate(args):
    """Deduplicate YAML questions in a directory and consolidate them."""
    source_dir = Path(args.directory)
    if not source_dir.is_dir():
        print(f"Error: Directory not found at '{source_dir}'")
        exit(1)
    output_file = Path(args.output_file) if args.output_file else source_dir / "unique_questions.yaml"
    loader = YAMLLoader()
    yaml_files = list(source_dir.rglob("*.yaml")) + list(source_dir.rglob("*.yml"))
    if not yaml_files:
        print(f"No YAML files found in '{source_dir}'.")
        return
    print(f"Found {len(yaml_files)} YAML files to process...")
    unique_questions: Dict[str, Question] = {}
    total_questions = 0
    duplicates_found = 0
    for file_path in yaml_files:
        try:
            questions = loader.load_file(str(file_path))
            total_questions += len(questions)
            for q in questions:
                key = _question_to_key(q)
                if key not in unique_questions:
                    unique_questions[key] = q
                else:
                    duplicates_found += 1
        except Exception as e:
            print(f"Warning: Could not process file {file_path}: {e}")
            continue
    print("\nScan complete.")
    print(f"  - Total questions found: {total_questions}")
    print(f"  - Duplicate questions found: {duplicates_found}")
    print(f"  - Unique questions: {len(unique_questions)}")
    if args.dry_run:
        print("\nDry run enabled. No files will be written.")
        return
    questions_for_yaml = [asdict(q) for q in unique_questions.values()]
    cleaned_questions_for_yaml = []
    for q_dict in questions_for_yaml:
        cleaned_questions_for_yaml.append({k: v for k, v in q_dict.items() if v is not None})
    output_data = {"questions": cleaned_questions_for_yaml}
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(output_data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
        print(f"\nSuccessfully wrote {len(unique_questions)} unique questions to '{output_file}'.")
    except IOError as e:
        print(f"\nError writing to output file '{output_file}': {e}")
        exit(1)


# --- diff_yaml_backups ---
def _compare_questions(questions1: List[Question], questions2: List[Question]):
    """Compares two lists of Question objects and prints the differences."""
    q1_map = {q.id: q for q in questions1}
    q2_map = {q.id: q for q in questions2}
    added_ids = q2_map.keys() - q1_map.keys()
    removed_ids = q1_map.keys() - q2_map.keys()
    common_ids = q1_map.keys() & q2_map.keys()
    modified_ids = []
    for q_id in common_ids:
        if str(q1_map[q_id]) != str(q2_map[q_id]):
            modified_ids.append(q_id)
    if added_ids:
        print(f"--- Added ({len(added_ids)}) ---")
        for q_id in sorted(list(added_ids)): print(f"  + {q_id}")
    if removed_ids:
        print(f"--- Removed ({len(removed_ids)}) ---")
        for q_id in sorted(list(removed_ids)): print(f"  - {q_id}")
    if modified_ids:
        print(f"--- Modified ({len(modified_ids)}) ---")
        for q_id in sorted(modified_ids): print(f"  ~ {q_id}")
    if not any([added_ids, removed_ids, modified_ids]):
        print("No changes detected.")
    print("-" * 20)

def do_diff(args):
    """Diff YAML backup files to track changes in questions."""
    loader = YAMLLoader()
    if len(args.files) == 2:
        path1, path2 = Path(args.files[0]), Path(args.files[1])
        if not path1.is_file() or not path2.is_file():
            print("Error: One or both files not found.", file=sys.stderr)
            sys.exit(1)
        print(f"Comparing {path1.name} to {path2.name}...")
        questions1 = loader.load_file(str(path1))
        questions2 = loader.load_file(str(path2))
        _compare_questions(questions1, questions2)
    elif len(args.files) == 0:
        print(f"No files specified. Discovering backups in: {', '.join(YAML_BACKUP_DIRS)}")
        try:
            all_files = find_yaml_files_from_paths(YAML_BACKUP_DIRS)
        except Exception as e:
            print(f"Error scanning directories: {e}", file=sys.stderr)
            sys.exit(1)
        if len(all_files) < 2:
            print("Not enough backup files found to compare.", file=sys.stderr)
            sys.exit(1)
        sorted_files = sorted(all_files, key=lambda p: p.stat().st_mtime)
        files_to_compare = sorted_files
        if args.range.lower() != 'all':
            try:
                num = int(args.range)
                if num < 1: raise ValueError
                if len(sorted_files) > num: files_to_compare = sorted_files[-(num + 1):]
            except (ValueError, TypeError):
                print(f"Invalid range: '{args.range}'. Please provide a positive integer or 'all'.", file=sys.stderr)
                sys.exit(1)
        if len(files_to_compare) < 2:
            print("Not enough backup files in the specified range to compare.", file=sys.stderr)
            sys.exit(1)
        print(f"Found {len(sorted_files)} backups. Comparing {len(files_to_compare) - 1} version(s) sequentially...")
        for i in range(len(files_to_compare) - 1):
            path1, path2 = files_to_compare[i], files_to_compare[i + 1]
            print(f"\nComparing {path1.name} -> {path2.name}")
            questions1 = loader.load_file(str(path1))
            questions2 = loader.load_file(str(path2))
            _compare_questions(questions1, questions2)
    else:
        print("Please provide either two files to compare, or no files to compare all backups.", file=sys.stderr)
        sys.exit(1)


# --- export_db_to_yaml ---
def _export_db_to_yaml_func(output_file: str, db_path: Optional[str] = None) -> int:
    """Exports questions from the database to a YAML file."""
    conn = get_db_connection(db_path=db_path)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute("SELECT * FROM questions")
    rows = cur.fetchall()
    conn.close()
    data = [_row_to_question_dict(row) for row in rows]
    with open(output_file, "w", encoding="utf-8") as f:
        yaml.safe_dump(data, f, default_flow_style=False, sort_keys=False)
    print(f"Exported {len(data)} questions to {output_file}")
    return len(data)

def do_export(args):
    """Export questions DB to YAML."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    if args.output:
        out_file = args.output
    else:
        out_dir = YAML_BACKUP_DIRS[0] if YAML_BACKUP_DIRS else "backups"
        os.makedirs(out_dir, exist_ok=True)
        out_file = os.path.join(out_dir, f"questions_{timestamp}.yaml")
    _export_db_to_yaml_func(out_file)


# --- import_from_yaml_with_ai ---
def do_import_ai(args):
    """Import questions from YAML files into a new SQLite database with AI-powered categorization."""
    if os.path.exists(args.output_db):
        overwrite = input(f"{Fore.YELLOW}Warning: Output database '{args.output_db}' already exists. Overwrite? (y/n): {Style.RESET_ALL}").lower()
        if overwrite != 'y':
            print("Operation cancelled.")
            return
        os.remove(args.output_db)
    try:
        categorizer = AICategorizer()
    except (ImportError, ValueError) as e:
        print(f"{Fore.RED}Failed to initialize AI Categorizer: {e}{Style.RESET_ALL}")
        sys.exit(1)
    print(f"Initializing new database at: {args.output_db}")
    init_db(db_path=args.output_db)
    conn = get_db_connection(db_path=args.output_db)
    search_dirs = args.search_dir or get_all_question_dirs()
    yaml_files = find_yaml_files(search_dirs)
    if not yaml_files:
        print(f"{Fore.YELLOW}No YAML files found in the specified directories.{Style.RESET_ALL}")
        return
    print(f"Found {len(yaml_files)} YAML file(s) to process...")
    all_questions = []
    loader = YAMLLoader()
    for file_path in yaml_files:
        try:
            questions = loader.load_file(str(file_path))
            all_questions.extend(questions)
        except Exception as e:
            print(f"{Fore.YELLOW}Warning: Could not process file '{file_path}': {e}{Style.RESET_ALL}")
    unique_questions: Dict[str, Question] = {}
    for q in all_questions:
        if q.prompt and q.prompt not in unique_questions:
            unique_questions[q.prompt] = q
    print(f"Found {len(unique_questions)} unique questions. Categorizing with AI...")
    processed_count = 0
    try:
        cursor = conn.cursor()
        with tqdm(total=len(unique_questions), desc="Categorizing Questions") as pbar:
            for question in unique_questions.values():
                q_dict = asdict(question)
                q_id = q_dict.get('id')
                prompt = q_dict.get('prompt')
                ai_categories = categorizer.categorize_question(q_dict)
                category_id = None
                subject_id = q_dict.get('category')
                if ai_categories:
                    category_id = ai_categories.get('exercise_category', category_id)
                    subject_id = ai_categories.get('subject_matter', subject_id)
                add_question(
                    conn=conn, id=q_id, prompt=prompt, source_file=q_dict.get('source_file'),
                    response=q_dict.get('response'), category=subject_id, source=q_dict.get('source'),
                    validation_steps=q_dict.get('validation_steps'), validator=q_dict.get('validator'),
                    review=q_dict.get('review', False)
                )
                if category_id:
                    cursor.execute("UPDATE questions SET category_id = ? WHERE id = ?", (category_id, q_id))
                processed_count += 1
                pbar.update(1)
        conn.commit()
    finally:
        if conn: conn.close()
    print(f"\n{Fore.GREEN}Successfully processed and added {processed_count} questions to '{args.output_db}'.{Style.RESET_ALL}")


# --- index_yaml_files ---
INDEX_FILE_PATH = project_root / "backups" / "index.yaml"

def _get_file_metadata(path: Path) -> dict:
    """Gathers metadata for a given file."""
    stat = path.stat()
    return {
        "path": str(path.relative_to(project_root)), "size_bytes": stat.st_size,
        "last_modified": datetime.datetime.fromtimestamp(stat.st_mtime).isoformat(),
    }

def do_index(args):
    """Finds all YAML files and creates an index file with their metadata."""
    print(f"{Fore.CYAN}--- Indexing all YAML files in repository ---{Style.RESET_ALL}")
    try:
        all_files = find_yaml_files(QUESTION_DIRS)
        if not all_files:
            all_files = get_all_yaml_files_in_repo()
        print("Directories scanned for YAML files:")
        for d in QUESTION_DIRS: print(f"  {d}")
        print("YAML files to index:")
        for p in all_files: print(f"  {p}")
        if not all_files:
            print(f"{Fore.YELLOW}No YAML files found to index in QUESTION_DIRS or repository.{Style.RESET_ALL}")
            return
        print(f"Found {len(all_files)} YAML files to index.")
        index_data = {
            "last_updated": datetime.datetime.now().isoformat(),
            "files": [_get_file_metadata(p) for p in all_files],
        }
        INDEX_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(INDEX_FILE_PATH, "w") as f:
            yaml.safe_dump(index_data, f, indent=2)
        print(f"{Fore.GREEN}Successfully created YAML index at: {INDEX_FILE_PATH}{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.RED}An error occurred: {e}{Style.RESET_ALL}")


# --- initialize_from_yaml ---
def _categorize_with_gemini(prompt: str) -> dict:
    """Uses llm-gemini to categorize a question."""
    try:
        result = subprocess.run(
            ["llm", "-m", "gemini-2.0-flash", "-o", "json_object", f"Categorize: {prompt}"],
            check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        return json.loads(result.stdout)
    except Exception as e:
        logging.error(f"Error categorizing with llm-gemini: {e}")
        return {}

def do_init(args):
    """Initializes the application database from consolidated YAML backups."""
    logging.info("Starting database initialization from consolidated YAML backups...")
    try:
        root = get_project_root()
        conn = get_db_connection()
    except Exception as e:
        logging.error(f"Error during initial setup: {e}")
        return
    backup_dir = root / 'yaml' / 'consolidated_backup'
    if not backup_dir.is_dir():
        logging.warning(f"Consolidated backup directory not found: {backup_dir}")
        return
    yaml_files = sorted(list(backup_dir.glob('**/*.yaml')) + list(backup_dir.glob('**/*.yml')))
    if not yaml_files:
        logging.info(f"No YAML files found in {backup_dir}.")
        return
    logging.info(f"Found {len(yaml_files)} YAML files to process in {backup_dir}.")
    cursor = conn.cursor()
    try:
        logging.info("Clearing existing questions from the database.")
        retry_count = 0
        while retry_count < 5:
            try:
                cursor.execute("DELETE FROM questions")
                break
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e):
                    logging.warning("Database is locked. Retrying...")
                    retry_count += 1
                    time.sleep(1)
                else: raise
        else:
            logging.error("Failed to clear 'questions' table after multiple retries. Aborting.")
            conn.close()
            return
    except sqlite3.Error as e:
        logging.error(f"Failed to clear 'questions' table: {e}. Aborting.")
        conn.close()
        return
    question_count = 0
    for yaml_file in yaml_files:
        relative_path = yaml_file.relative_to(root)
        logging.info(f"Processing file: {relative_path}")
        try:
            with open(yaml_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            if not isinstance(data, list):
                logging.warning(f"YAML file {relative_path} does not contain a list of questions. Skipping.")
                continue
            for question_data in data:
                if not isinstance(question_data, dict):
                    logging.warning(f"Skipping non-dictionary item in {relative_path}")
                    continue
                prompt = question_data.get('prompt')
                if not prompt:
                    logging.warning(f"Skipping question with no prompt in {relative_path}")
                    continue
                logging.info(f"Categorizing question with llm-gemini: '{prompt[:70]}...'")
                categories = _categorize_with_gemini(prompt)
                exercise_category = categories.get('exercise_category', 'custom')
                subject_matter = categories.get('subject_matter', 'Unknown')
                q_id = question_data.get('id', str(uuid.uuid4()))
                sql = "INSERT INTO questions (id, prompt, response, source_file, category_id, subject_id) VALUES (?, ?, ?, ?, ?, ?)"
                params = (q_id, prompt, question_data.get('response'), str(relative_path), exercise_category, subject_matter)
                retry_count = 0
                while retry_count < 5:
                    try:
                        cursor.execute(sql, params)
                        break
                    except sqlite3.OperationalError as e:
                        if "database is locked" in str(e):
                            logging.warning("Database is locked. Retrying...")
                            retry_count += 1
                            time.sleep(1)
                        else: raise
                else:
                    logging.error(f"Failed to insert question {q_id} after multiple retries. Skipping.")
                    continue
                question_count += 1
        except yaml.YAMLError as e:
            logging.error(f"Error parsing YAML file {relative_path}: {e}")
        except sqlite3.Error as e:
            logging.error(f"Database error while processing a question from {relative_path}: {e}")
        except Exception as e:
            logging.error(f"An unexpected error occurred while processing {relative_path}: {e}")
    conn.commit()
    conn.close()
    logging.info(f"Database initialization complete. Loaded {question_count} questions.")


# --- restore_yaml_to_db ---
def _restore_yaml_to_db_func(yaml_files: list[Path], clear_db: bool, db_path: Optional[str] = None):
    """Restores questions from a list of YAML files to the SQLite database."""
    console = Console()
    if clear_db:
        console.print("[bold yellow]Clearing existing database...[/bold yellow]")
        init_db(clear=True, db_path=db_path)
    else:
        init_db(clear=False, db_path=db_path)
    total_questions, errors = 0, 0
    conn = get_db_connection(db_path=db_path)
    try:
        for path in track(yaml_files, description="Processing YAML files..."):
            console.print(f"  - Processing '{path.name}'...")
            try:
                with open(path, "r") as f:
                    data = yaml.safe_load(f)
                if data is None: continue
                questions_list = []
                if isinstance(data, list): questions_list = data
                elif isinstance(data, dict):
                    if "questions" in data: questions_list = data.get("questions", [])
                    elif "id" in data: questions_list = [data]
                if not questions_list:
                    console.print(f"    [yellow]Skipping file with no questions: {path.name}[/yellow]")
                    continue
                for q_data in questions_list:
                    if not isinstance(q_data, dict): continue
                    try:
                        if "metadata" in q_data and isinstance(q_data.get("metadata"), dict):
                            for key, value in q_data["metadata"].items():
                                if key not in q_data: q_data[key] = value
                        if "question" in q_data and "prompt" not in q_data: q_data["prompt"] = q_data.pop("question")
                        if "answer" in q_data and "response" not in q_data: q_data["response"] = q_data.pop("answer")
                        if "citation" in q_data and "source" not in q_data: q_data["source"] = q_data.pop("citation")
                        if not all(k in q_data for k in ["id", "prompt"]):
                            console.print(f"[red]Error in {path.name}: Skipping question missing 'id' or 'prompt'. ID: {q_data.get('id', 'N/A')}[/red]")
                            errors += 1
                            continue
                        q_data["source_file"] = str(path)
                        if "type" in q_data: q_data["question_type"] = q_data.pop("type")
                        if "subject" in q_data: q_data["subject_matter"] = q_data.pop("subject")
                        valid_keys = {
                            "id", "prompt", "response", "category", "source", "validation_steps",
                            "validator", "source_file", "review", "explanation", "difficulty",
                            "pre_shell_cmds", "initial_files", "question_type", "answers",
                            "correct_yaml", "schema_category", "metadata", "subject_matter",
                        }
                        kwargs_for_add = {key: q_data[key] for key in valid_keys if key in q_data}
                        add_question(conn=conn, **kwargs_for_add)
                        total_questions += 1
                    except Exception as e:
                        console.print(f"[red]Error adding question from {path.name} ({q_data.get('id', 'N/A')}): {e}[/red]")
                        errors += 1
            except yaml.YAMLError as e:
                console.print(f"[red]Error parsing YAML file {path.name}: {e}[/red]")
                errors += 1
            except Exception as e:
                console.print(f"[red]An unexpected error occurred with file {path.name}: {e}[/red]")
                errors += 1
    finally:
        conn.close()
    console.print(f"\n[bold green]Restore complete.[/bold green]")
    console.print(f"  - Total questions added: {total_questions}")
    if errors > 0:
        console.print(f"  - Errors encountered: {errors}")

def do_restore(args):
    """Restore questions from YAML files into the SQLite database."""
    console = Console()
    if args.paths:
        yaml_files = find_yaml_files_from_paths(args.paths)
    else:
        console.print("No input paths provided. Scanning default question directories...")
        default_dirs = get_all_question_dirs()
        yaml_files = find_yaml_files_from_paths(default_dirs)
    if not yaml_files:
        console.print("[bold red]No YAML files found to process.[/bold red]")
        sys.exit(1)
    console.print(f"Found {len(yaml_files)} YAML file(s) to process.")
    _restore_yaml_to_db_func(
        yaml_files=sorted(list(yaml_files)), clear_db=args.clear, db_path=args.db_path
    )


# --- show_previous_yaml_backups & show_yaml_backups ---
def do_list_backups(args):
    """Finds and prints all YAML backup files."""
    backup_dirs = YAML_BACKUP_DIRS
    if not backup_dirs:
        if not args.path_only:
            print("No YAML backup directories are configured.", file=sys.stderr)
        sys.exit(1)
    try:
        backup_files = find_and_sort_files_by_mtime(backup_dirs, [".yaml", ".yml"])
    except Exception as e:
        if not args.path_only:
            print(f"Error scanning directories: {e}", file=sys.stderr)
        sys.exit(1)
    if not backup_files:
        if not args.path_only:
            print("No YAML backup files found.")
        sys.exit(0)
    if args.path_only:
        for f in backup_files:
            print(f)
    else:
        print(f"Searching for YAML backup files in: {', '.join(backup_dirs)}...")
        print(f"\nFound {len(backup_files)} backup file(s), sorted by most recent:\n")
        for f in backup_files:
            mod_time = f.stat().st_mtime
            mod_time_str = datetime.datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')
            print(f"- {mod_time_str} | {f.name} ({f.parent})")


# --- write_db_from_yaml & restore_db_from_yaml ---
def do_write(args):
    """Write DB from YAML backup, with an option to backup existing DB."""
    if not os.path.exists(args.yaml_file):
        print('File not found:', args.yaml_file, file=sys.stderr)
        sys.exit(1)
    if args.backup:
        backup_path = DATABASE_FILE + '.pre_restore.bak'
        os.makedirs(os.path.dirname(backup_path), exist_ok=True)
        shutil.copy(DATABASE_FILE, backup_path)
        print(f'Backed up current database to {backup_path}')
    if args.overwrite:
        conn = get_db_connection()
        conn.execute('DELETE FROM questions')
        conn.commit()
        conn.close()
    with open(args.yaml_file) as f:
        data = yaml.safe_load(f) or []
    conn = get_db_connection()
    for q in data:
        try:
            add_question(conn=conn, **q)
        except Exception as e:
            print('Error importing question', q.get('id'), e, file=sys.stderr)
    conn.close()
    print(f'Loaded {len(data)} questions into the database.')


# --- yaml_backup_stats ---
EXERCISE_TYPE_TO_CATEGORY = {
    "socratic": "Basic", "command": "Command", "yaml_author": "Manifest",
    "yaml_edit": "Manifest", "live_k8s_edit": "Manifest",
}

def _analyze_file(path):
    """Analyzes a single YAML file and returns statistics about its questions."""
    loader = YAMLLoader()
    try:
        questions = loader.load_file(str(path))
    except Exception as e:
        return {'file': str(path), 'error': f'parse error: {e}'}
    total = len(questions)
    breakdown = {"Basic": Counter(), "Command": Counter(), "Manifest": Counter(), "Unknown": Counter()}
    for q in questions:
        ex_type = getattr(q, "type", "Unknown Type") or "Unknown"
        subject = (getattr(q, 'metadata', None) or {}).get('category', "Uncategorized") or "Uncategorized"
        if subject == "Uncategorized": subject = "Vim"
        major_category = EXERCISE_TYPE_TO_CATEGORY.get(ex_type, "Unknown")
        if subject in ["Resource Reference", "Kubectl Operations"]: major_category = "Basic"
        breakdown[major_category][subject] += 1
    breakdown = {k: v for k, v in breakdown.items() if v}
    breakdown_dict = {k: dict(v) for k, v in breakdown.items()}
    category_counts = {cat: sum(counts.values()) for cat, counts in breakdown.items()}
    size = path.stat().st_size
    mtime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(path.stat().st_mtime))
    return {
        'file': str(path), 'size': size, 'mtime': mtime, 'total': total,
        'category_counts': category_counts, 'breakdown': breakdown_dict,
    }

def do_backup_stats(args):
    """Show stats for the latest YAML backup file found in the given paths."""
    scan_paths = args.paths or YAML_BACKUP_DIRS
    try:
        files = find_yaml_files_from_paths(scan_paths)
    except Exception as e:
        print(f"Error scanning directories: {e}", file=sys.stderr)
        sys.exit(1)
    if args.pattern:
        regex = re.compile(args.pattern)
        files = [f for f in files if regex.search(str(f))]
    if not files:
        print(f"No YAML files found in {', '.join(scan_paths)}")
        sys.exit(0)
    latest_file = max(files, key=lambda f: f.stat().st_mtime)
    if not args.json:
        print(f"Found {len(files)} backup files. Analyzing latest: {latest_file}", file=sys.stderr)
    stats = [_analyze_file(latest_file)]
    if args.json:
        print(json.dumps(stats, indent=2))
    else:
        for s in stats:
            if 'error' in s:
                print(f"{s['file']}: {s['error']}")
            else:
                print(f"File: {s['file']}")
                print(f" Size: {s['size']} bytes  Modified: {s['mtime']}")
                print(f" Total questions: {s['total']}")
                print(" Questions by Exercise Category:")
                for category, count in sorted(s['category_counts'].items(), key=lambda x: -x[1]):
                    print(f"  {category}: {count}")
                    subject_counts = s.get('breakdown', {}).get(category, {})
                    for subject, sub_count in sorted(subject_counts.items(), key=lambda x: -x[1]):
                        print(f"    - {subject}: {sub_count}")
                print()


# --- yaml_statistics ---
def do_statistics(args):
    """Calculates and prints statistics about questions in YAML files."""
    loader = YAMLLoader()
    yaml_files: List[Path] = []
    if args.path:
        target_path = Path(args.path)
        if not target_path.exists():
            print(f"Error: Path does not exist: {target_path}", file=sys.stderr)
            sys.exit(1)
        if target_path.is_dir():
            print(f"Scanning for YAML files in: {target_path}")
            yaml_files = find_yaml_files([str(target_path)])
        elif target_path.is_file():
            if target_path.suffix.lower() not in ['.yaml', '.yml']:
                print(f"Error: Specified file is not a YAML file: {target_path}", file=sys.stderr)
                sys.exit(1)
            yaml_files = [target_path]
    else:
        search_dirs = QUESTION_DIRS
        print(f"No path specified. Searching in default question directories: {', '.join(search_dirs)}")
        yaml_files = find_yaml_files(search_dirs)
    if not yaml_files:
        print("No YAML files found to analyze.")
        return
    all_questions: List[Question] = []
    print(f"Found {len(yaml_files)} YAML file(s). Loading questions...")
    for file_path in yaml_files:
        try:
            questions_from_file = loader.load_file(str(file_path))
            if questions_from_file:
                all_questions.extend(questions_from_file)
        except Exception as e:
            print(f"Warning: Could not load or parse {file_path}: {e}", file=sys.stderr)
            continue
    if not all_questions:
        print("No questions could be loaded from the specified YAML files.")
        return
    type_counts = Counter(q.type for q in all_questions if hasattr(q, 'type') and q.type)
    category_counts = Counter(q.category for q in all_questions if hasattr(q, 'category') and q.category)
    print(f"\n--- YAML Question Statistics ---")
    print(f"Total Questions Found: {len(all_questions)}")
    print("\n--- Questions by Exercise Type ---")
    if type_counts:
        for q_type, count in type_counts.most_common(): print(f"  - {q_type:<20} {count}")
    else:
        print("  No questions with 'type' field found.")
    print("\n--- Questions by Subject Matter (Category) ---")
    if category_counts:
        for category, count in category_counts.most_common(): print(f"  - {category:<30} {count}")
    else:
        print("  No questions with 'category' field found.")


def main():
    """Main CLI entrypoint."""
    parser = argparse.ArgumentParser(
        description="A tool for managing Kubelingo's YAML files and database.",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", required=True, help="Available commands")

    # consolidate
    p_consolidate = subparsers.add_parser('consolidate', help="Consolidate unique YAML questions from across the repository into a single file.")
    p_consolidate.add_argument('-o', '--output', type=Path, help=f'Output file path for consolidated questions.')
    p_consolidate.set_defaults(func=do_consolidate)
    
    # create-quizzes
    p_create_quizzes = subparsers.add_parser('create-quizzes', help="Create quizzes from consolidated YAML backup.")
    p_create_quizzes.set_defaults(func=do_create_quizzes)
    
    # create-db
    p_create_db = subparsers.add_parser('create-db', help="Populate the SQLite database from YAML backup files.")
    p_create_db.add_argument("--yaml-files", nargs="*", type=str, help="Path(s) to input YAML file(s) or directories.")
    p_create_db.add_argument("--db-path", type=str, default=None, help="Path to the SQLite database file.")
    p_create_db.add_argument("--clear", action="store_true", help="Clear the database before populating.")
    p_create_db.set_defaults(func=do_create_db)
    
    # deduplicate
    p_deduplicate = subparsers.add_parser('deduplicate', help="Deduplicate YAML questions in a directory.")
    p_deduplicate.add_argument("directory", type=str, help="Directory containing YAML question files.")
    p_deduplicate.add_argument("-o", "--output-file", type=str, help="Output file for consolidated unique questions.")
    p_deduplicate.add_argument("--dry-run", action="store_true", help="Perform a dry run without writing files.")
    p_deduplicate.set_defaults(func=do_deduplicate)
    
    # diff
    p_diff = subparsers.add_parser('diff', help="Diff YAML backup files to track changes.")
    p_diff.add_argument('files', nargs='*', help="Two YAML files to compare. If not provided, compares all backups.")
    p_diff.add_argument("--range", help="Number of recent versions to diff (e.g., '5' for last 5). 'all' to diff all.", default="all")
    p_diff.set_defaults(func=do_diff)
    
    # export
    p_export = subparsers.add_parser('export', help="Export questions DB to YAML.")
    p_export.add_argument("-o", "--output", help="Output YAML file path.")
    p_export.set_defaults(func=do_export)

    # import-ai
    p_import_ai = subparsers.add_parser('import-ai', help="Import from YAML with AI categorization.")
    p_import_ai.add_argument("output_db", help="Path to the new SQLite database file to be created.")
    p_import_ai.add_argument("--search-dir", action='append', help="Directory to search for YAML files. Can be used multiple times.")
    p_import_ai.set_defaults(func=do_import_ai)

    # index
    p_index = subparsers.add_parser('index', help="Finds all YAML files and creates an index file with their metadata.")
    p_index.set_defaults(func=do_index)
    
    # init
    p_init = subparsers.add_parser('init', help="Initializes the database from consolidated YAML backups.")
    p_init.set_defaults(func=do_init)
    
    # restore
    p_restore = subparsers.add_parser('restore', help="Restore questions from YAML into the database.")
    p_restore.add_argument("paths", nargs='*', help="Paths to YAML files or directories. Scans default dirs if not provided.")
    p_restore.add_argument('--clear', action='store_true', help="Clear the existing database before restoring.")
    p_restore.add_argument('--db-path', type=str, default=None, help="Path to the SQLite database file.")
    p_restore.set_defaults(func=do_restore)

    # list-backups
    p_list_backups = subparsers.add_parser('list-backups', help='Finds and displays all YAML backup files.')
    p_list_backups.add_argument("--path-only", action="store_true", help="Only prints the paths of the files.")
    p_list_backups.set_defaults(func=do_list_backups)
    
    # write
    p_write = subparsers.add_parser('write', help="Write DB from YAML backup.")
    p_write.add_argument('yaml_file', help='YAML file to load.')
    p_write.add_argument('--overwrite', action='store_true', help='Overwrite existing questions.')
    p_write.add_argument('--backup', action='store_true', help='Backup current DB before writing.')
    p_write.set_defaults(func=do_write)

    # backup-stats
    p_backup_stats = subparsers.add_parser('backup-stats', help="Show stats for the latest YAML backup file.")
    p_backup_stats.add_argument('paths', nargs='*', help='Path(s) to YAML file(s) or directories.')
    p_backup_stats.add_argument('-p', '--pattern', help='Regex to filter filenames')
    p_backup_stats.add_argument('--json', action='store_true', help='Output stats in JSON format')
    p_backup_stats.set_defaults(func=do_backup_stats)
    
    # statistics
    p_stats = subparsers.add_parser('stats', help="Get statistics about questions in YAML files.")
    p_stats.add_argument("path", nargs='?', default=None, help="Path to a YAML file or directory.")
    p_stats.set_defaults(func=do_statistics)

    args = parser.parse_args()
    args.func(args)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    main()
